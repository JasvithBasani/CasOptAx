{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8fX62g9Ac5J",
        "outputId": "8d1d8b4d-aeff-4bc2-8aad-65c0c03c75e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax) (0.1.86)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.10/dist-packages (from optax) (0.4.23)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from optax) (0.4.23+cuda12.cudnn89)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from optax) (1.25.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax) (4.10.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax) (1.11.4)\n",
            "Cloning into 'CasOptAx'...\n",
            "remote: Enumerating objects: 143, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 143 (delta 0), reused 0 (delta 0), pack-reused 142\u001b[K\n",
            "Receiving objects: 100% (143/143), 46.07 KiB | 873.00 KiB/s, done.\n",
            "Resolving deltas: 100% (82/82), done.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "import jax\n",
        "from jax import config\n",
        "config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "import jax.numpy as jnp\n",
        "from jax import jit, value_and_grad\n",
        "from functools import partial\n",
        "from itertools import product\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import unitary_group\n",
        "\n",
        "!pip install optax\n",
        "import optax\n",
        "\n",
        "!git clone https://github.com/JasvithBasani/CasOptAx.git\n",
        "import CasOptAx as conn\n",
        "from CasOptAx.linear_optics import Linear_Optics\n",
        "from CasOptAx.circuit_builder import Circuit_singlemode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29Z9eNFpAtbG",
        "outputId": "a0a3a2b1-e8b3-4995-a131-1c7e4c3cceb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialzing Circuit, Please Wait .  .  .\n",
            "***Circuit Ready For Compilation***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/CasOptAx/scatterer.py:26: UserWarning: Ensure that all frequency arrays are consistant, default k = jnp.arange(-6, 6, 70)\n",
            "  warnings.warn('Ensure that all frequency arrays are consistant, default k = jnp.arange(-6, 6, 70)')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{(0, 0, 0, 3): 0j,\n",
              " (0, 0, 1, 2): 0j,\n",
              " (0, 0, 2, 1): 0j,\n",
              " (0, 0, 3, 0): 0j,\n",
              " (0, 1, 0, 2): 0j,\n",
              " (0, 1, 1, 1): 0j,\n",
              " (0, 1, 2, 0): 0j,\n",
              " (0, 2, 0, 1): 0j,\n",
              " (0, 2, 1, 0): 0j,\n",
              " (0, 3, 0, 0): 0j,\n",
              " (1, 0, 0, 2): 0j,\n",
              " (1, 0, 1, 1): 0j,\n",
              " (1, 0, 2, 0): 0j,\n",
              " (1, 1, 0, 1): 0j,\n",
              " (1, 1, 1, 0): 0j,\n",
              " (1, 2, 0, 0): 0j,\n",
              " (2, 0, 0, 1): 0j,\n",
              " (2, 0, 1, 0): 0j,\n",
              " (2, 1, 0, 0): 0j,\n",
              " (3, 0, 0, 0): (1+0j)}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Define the number of photons in the network\n",
        "N_photons = 3\n",
        "#Define the number of modes/waveguides in the network\n",
        "N_modes = 4\n",
        "#Define the number of layers in the network\n",
        "N_layers = 2\n",
        "\n",
        "#Let us initialize the singlemode circuit class with a randomly select N_photon, N_mode state\n",
        "#Here, 'singlemode' refers to the retention of the gaussian spectral profile after all the scattering processes\n",
        "input_photons = tuple([N_photons] + [0] * (N_modes - 1))\n",
        "circ = Circuit_singlemode(N_modes, N_photons, input_photons)\n",
        "\n",
        "#The N_photon, N_mode states in the circ class have a very particular structure. This structure lies in the ordering of the states\n",
        "#Let us look at this state\n",
        "init_state = circ.state_amps.copy()\n",
        "init_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jl0KtWiCn5l",
        "outputId": "5162f23c-142b-4998-8665-ed133e1c1d5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{(0, 0, 0, 3): Array(-0.06048939-0.07918852j, dtype=complex128),\n",
              " (0, 0, 1, 2): Array(-0.20830109+0.40231447j, dtype=complex128),\n",
              " (0, 0, 2, 1): Array(-0.11191342-0.22556897j, dtype=complex128),\n",
              " (0, 0, 3, 0): Array(0.07679562+0.08252124j, dtype=complex128),\n",
              " (0, 1, 0, 2): Array(0.00054541-0.02020634j, dtype=complex128),\n",
              " (0, 1, 1, 1): Array(-0.26533795+0.01508944j, dtype=complex128),\n",
              " (0, 1, 2, 0): Array(-0.069569+0.11455554j, dtype=complex128),\n",
              " (0, 2, 0, 1): Array(0.06269142+0.05989441j, dtype=complex128),\n",
              " (0, 2, 1, 0): Array(-0.08216262-0.09862449j, dtype=complex128),\n",
              " (0, 3, 0, 0): Array(0.07306662-0.41923714j, dtype=complex128),\n",
              " (1, 0, 0, 2): Array(0.1814922+0.00389952j, dtype=complex128),\n",
              " (1, 0, 1, 1): Array(0.02801461-0.26674467j, dtype=complex128),\n",
              " (1, 0, 2, 0): Array(0.06081417+0.20706778j, dtype=complex128),\n",
              " (1, 1, 0, 1): Array(-0.22801683-0.16676589j, dtype=complex128),\n",
              " (1, 1, 1, 0): Array(-0.18186234-0.13296931j, dtype=complex128),\n",
              " (1, 2, 0, 0): Array(-0.08905806-0.16178644j, dtype=complex128),\n",
              " (2, 0, 0, 1): Array(-0.15890232-0.08870878j, dtype=complex128),\n",
              " (2, 0, 1, 0): Array(0.14738443-0.06496988j, dtype=complex128),\n",
              " (2, 1, 0, 0): Array(-0.00837978+0.17990557j, dtype=complex128),\n",
              " (3, 0, 0, 0): Array(-0.06118607-0.05692335j, dtype=complex128)}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#In this tutorial, we want to train the QPNN to map an input state to a Haar-random state\n",
        "#Let us generate this Haar-random state. First, we need to know the dimension in the qudit basis\n",
        "N_dim = circ.num_possible_states\n",
        "U_haar = jnp.array(unitary_group.rvs(circ.num_possible_states), dtype = jnp.complex128)\n",
        "target_state = init_state.copy() #To maintain the structure of the pytree\n",
        "\n",
        "input_amps = []\n",
        "for idx, s in enumerate(list(init_state.keys())):\n",
        "  input_amps.append(init_state[s])\n",
        "output_amps = jnp.matmul(U_haar, jnp.array(input_amps))\n",
        "\n",
        "for idx, s in enumerate(list(init_state.keys())):\n",
        "  target_state[s] = output_amps[idx]\n",
        "\n",
        "target_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nu4esyF-D8zy"
      },
      "outputs": [],
      "source": [
        "#Now we define the parameters of the network\n",
        "lo = Linear_Optics(N_modes)\n",
        "\n",
        "theta, phi, D = [], [], []\n",
        "chi_1, chi_2 = [], []\n",
        "\n",
        "#Initialize the network randomly. Each layer is initialized to a Haar-random matrix, approximated by the clements decomposition\n",
        "for layer in range(N_layers):\n",
        "  U = lo.haar_mat(N_modes) + 0j\n",
        "  theta_, phi_, D_ = lo.get_clements_phases(U)\n",
        "  theta.append(theta_); phi.append(phi_); D.append(D_)\n",
        "  chi_1_val = jnp.array(np.random.randn(N_modes))\n",
        "  chi_2_val = jnp.array(np.random.randn(N_modes))\n",
        "  chi_1.append(chi_1_val); chi_2.append(chi_2_val)\n",
        "\n",
        "#Typecast everything into jax arrays\n",
        "theta, phi, D = jnp.array(theta), jnp.array(phi), jnp.array(D)\n",
        "chi_1, chi_2 = jnp.array(chi_1), jnp.array(chi_2)\n",
        "#alpha and beta are the beam-splitter errors, i.e., deviation from 50:50 splitting. Assumed to be zero for now\n",
        "alpha = jnp.zeros(theta[0].shape)\n",
        "beta = alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hRUHeD7iFtYX"
      },
      "outputs": [],
      "source": [
        "#QPNN function defined here - cascaded linear and 3-level system nonlinearities\n",
        "def QPNN(theta, phi, D, chi_1, chi_2, amps):\n",
        "  for l_num in range(N_layers):\n",
        "    amps = circ.add_linear_layer(amps, theta[l_num], phi[l_num], D[l_num], alpha, beta)\n",
        "    amps = circ.add_3ls_nonlinear_layer(amps, chi_1[l_num], chi_2[l_num])\n",
        "  return amps\n",
        "\n",
        "out_state = QPNN(theta, phi, D, chi_1, chi_2, init_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPj32j6RF6kd",
        "outputId": "fe5a77df-cac7-4c0b-9782-33645d4db4c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Array(0.69835682, dtype=float64), Array(0.16432254, dtype=float64))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Define the loss function\n",
        "def loss_func(theta, phi, D, chi_1, chi_2, init_state, target_state):\n",
        "  out_state = QPNN(theta, phi, D, chi_1, chi_2, init_state)\n",
        "\n",
        "  @jit\n",
        "  def MSE(out_state, target_state):\n",
        "    #mse_val = jax.tree_map(lambda amp_1, amp_2: jnp.abs(amp_1 - amp_2)**2, out_state, target_state)\n",
        "    mse_val, _ = jax.flatten_util.ravel_pytree(jax.tree_map(lambda amp_1, amp_2: jnp.abs(amp_1 - amp_2)**2, out_state, target_state))\n",
        "    return jnp.mean(mse_val)\n",
        "\n",
        "  @jit\n",
        "  def dot(out_state, target_state):\n",
        "    #dot_prod = jax.tree_map(lambda amp_1, amp_2: amp_1 * amp_2, out_state, target_state)\n",
        "    dot_val, _ = jax.flatten_util.ravel_pytree(jax.tree_map(lambda amp_1, amp_2: amp_1 * amp_2, out_state, target_state))\n",
        "    return jnp.abs(jnp.sum(dot_val))\n",
        "\n",
        "  #Fidelity defined as inner product of the output state and target state\n",
        "  fid_val = dot(out_state, target_state)\n",
        "  #Loss minimized to maximize fidelity to 1\n",
        "  loss_val = jnp.abs(1 - fid_val)**2\n",
        "  return loss_val, (fid_val)\n",
        "\n",
        "loss_out = loss_func(theta, phi, D, chi_1, chi_2, init_state, target_state)\n",
        "loss_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2dN_ZvzGX7j",
        "outputId": "d2057904-6733-445e-c34e-6dbcaabfb9ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6983568212157405 0.16432253756862605\n"
          ]
        }
      ],
      "source": [
        "#Define training hyperparameters - number of epochs, learning rate scheduling\n",
        "num_epochs = 2001\n",
        "scheduler = optax.exponential_decay(init_value=0.025, transition_steps = num_epochs, decay_rate=0.1)\n",
        "optimizer = optax.adam(scheduler)\n",
        "\n",
        "def update(step, theta, phi, D, chi_1, chi_2, opt_state, in_state, target_state):\n",
        "  #Calculate gradients of parameters 0-4\n",
        "  (loss_val, (fid_val)), grads = value_and_grad(loss_func, (0, 1, 2, 3, 4), has_aux = True)(theta, phi, D, chi_1, chi_2, in_state, target_state)\n",
        "  #Perform updates on the parameters\n",
        "  updates, opt_state = optimizer.update(grads, opt_state)\n",
        "  theta, phi, D, chi_1, chi_2 = optax.apply_updates((theta, phi, D, chi_1, chi_2), updates)\n",
        "  return loss_val, fid_val, opt_state, (theta, phi, D, chi_1, chi_2)\n",
        "\n",
        "#Initialize optimizer only for the parameters to be optimized\n",
        "opt_state = optimizer.init((theta, phi, D, chi_1, chi_2))\n",
        "loss_val, fid_val, opt_state, (theta, phi, D, chi_1, chi_2) = update(0, theta, phi, D, chi_1, chi_2, opt_state, init_state, target_state)\n",
        "print (loss_val, fid_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_JTnifAHMoY",
        "outputId": "1a2f4972-77fa-47b8-aba2-cf342b44a8ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch Number: 0, Loss: 0.22792792725037958, Fidelity: 0.5225820203947283\n",
            "Epoch Number: 20, Loss: 0.06065023133719564, Fidelity: 0.7537273232021148\n",
            "Epoch Number: 40, Loss: 0.02562745866099215, Fidelity: 0.8399142146816522\n",
            "Epoch Number: 60, Loss: 0.017270383629769202, Fidelity: 0.8685831683924422\n",
            "Epoch Number: 80, Loss: 0.014167350352914585, Fidelity: 0.8809733208355598\n",
            "Epoch Number: 100, Loss: 0.011603122777556825, Fidelity: 0.8922822077019918\n",
            "Epoch Number: 120, Loss: 0.009333294235172328, Fidelity: 0.9033910240444899\n",
            "Epoch Number: 140, Loss: 0.007717416622725001, Fidelity: 0.9121511717623678\n",
            "Epoch Number: 160, Loss: 0.006664820225059984, Fidelity: 0.9183616497896976\n",
            "Epoch Number: 180, Loss: 0.005913012294725586, Fidelity: 0.9231038863483623\n",
            "Epoch Number: 200, Loss: 0.005309700649862288, Fidelity: 0.9271323072283588\n",
            "Epoch Number: 220, Loss: 0.004783115366323919, Fidelity: 0.9308399293932986\n",
            "Epoch Number: 240, Loss: 0.004302197140667781, Fidelity: 0.9344088638559463\n",
            "Epoch Number: 260, Loss: 0.003859977226958529, Fidelity: 0.9378712850047699\n",
            "Epoch Number: 280, Loss: 0.0034616714806691724, Fidelity: 0.9411640290241661\n",
            "Epoch Number: 300, Loss: 0.0031138043737444776, Fidelity: 0.9441985271364234\n",
            "Epoch Number: 320, Loss: 0.0028179447515522733, Fidelity: 0.9469156826213968\n",
            "Epoch Number: 340, Loss: 0.002570303725844702, Fidelity: 0.9493018370565096\n",
            "Epoch Number: 360, Loss: 0.00236419012105601, Fidelity: 0.9513770617809247\n",
            "Epoch Number: 380, Loss: 0.002192336049846471, Fidelity: 0.9531776116601633\n",
            "Epoch Number: 400, Loss: 0.0020481526103690454, Fidelity: 0.9547434799131767\n",
            "Epoch Number: 420, Loss: 0.0019261495317615274, Fidelity: 0.9561120798879518\n",
            "Epoch Number: 440, Loss: 0.0018219331250774887, Fidelity: 0.957315891422246\n",
            "Epoch Number: 460, Loss: 0.0017320501455409523, Fidelity: 0.9583820934507639\n",
            "Epoch Number: 480, Loss: 0.0016538031201027553, Fidelity: 0.9593330217485642\n",
            "Epoch Number: 500, Loss: 0.0015850842056583491, Fidelity: 0.9601868839996371\n",
            "Epoch Number: 520, Loss: 0.001524239699291711, Fidelity: 0.9609584874871412\n",
            "Epoch Number: 540, Loss: 0.001469964245930706, Fidelity: 0.9616598872467659\n",
            "Epoch Number: 560, Loss: 0.0014212198184035808, Fidelity: 0.962300930801894\n",
            "Epoch Number: 580, Loss: 0.0013771742182237259, Fidelity: 0.9628897019922539\n",
            "Epoch Number: 600, Loss: 0.0013371544977554958, Fidelity: 0.9634328768187119\n",
            "Epoch Number: 620, Loss: 0.0013006116699170354, Fidelity: 0.9639360059073175\n",
            "Epoch Number: 640, Loss: 0.0012670938988907578, Fidelity: 0.9644037375713298\n",
            "Epoch Number: 660, Loss: 0.001236226067953535, Fidelity: 0.9648399933453713\n",
            "Epoch Number: 680, Loss: 0.0012076941268678093, Fidelity: 0.9652481061398402\n",
            "Epoch Number: 700, Loss: 0.0011812330421646518, Fidelity: 0.9656309289889201\n",
            "Epoch Number: 720, Loss: 0.0011566174587824432, Fidelity: 0.9659909209359847\n",
            "Epoch Number: 740, Loss: 0.0011336544165405223, Fidelity: 0.9663302150802753\n",
            "Epoch Number: 760, Loss: 0.001112177627693185, Fidelity: 0.9666506727550137\n",
            "Epoch Number: 780, Loss: 0.0010920429428519013, Fidelity: 0.9669539269677636\n",
            "Epoch Number: 800, Loss: 0.0010731247312871332, Fidelity: 0.967241417440812\n",
            "Epoch Number: 820, Loss: 0.0010553129668104107, Fidelity: 0.967514419093844\n",
            "Epoch Number: 840, Loss: 0.0010385108591051688, Fidelity: 0.9677740654269706\n",
            "Epoch Number: 860, Loss: 0.001022632916153349, Fidelity: 0.9680213678192242\n",
            "Epoch Number: 880, Loss: 0.0010076033442003736, Fidelity: 0.9682572316235591\n",
            "Epoch Number: 900, Loss: 0.0009933547153700363, Fidelity: 0.9684824697133478\n",
            "Epoch Number: 920, Loss: 0.0009798268526982096, Fidelity: 0.9686978139310014\n",
            "Epoch Number: 940, Loss: 0.000966965889332228, Fidelity: 0.9689039248564674\n",
            "Epoch Number: 960, Loss: 0.0009547234685094409, Fidelity: 0.9691014002176565\n",
            "Epoch Number: 980, Loss: 0.0009430560628317322, Fidelity: 0.9692907821195047\n",
            "Epoch Number: 1000, Loss: 0.0009319243881124699, Fidelity: 0.9694725633550331\n",
            "Epoch Number: 1020, Loss: 0.0009212929003166735, Fidelity: 0.9696471928758365\n",
            "Epoch Number: 1040, Loss: 0.0009111293584864154, Fidelity: 0.9698150806115634\n",
            "Epoch Number: 1060, Loss: 0.0009014044470939242, Fidelity: 0.9699766016731296\n",
            "Epoch Number: 1080, Loss: 0.0008920914461289905, Fidelity: 0.9701321000716657\n",
            "Epoch Number: 1100, Loss: 0.0008831659435910309, Fidelity: 0.9702818919917329\n",
            "Epoch Number: 1120, Loss: 0.0008746055838709536, Fidelity: 0.9704262686853526\n",
            "Epoch Number: 1140, Loss: 0.0008663898464722356, Fidelity: 0.9705654990449603\n",
            "Epoch Number: 1160, Loss: 0.0008584998530097426, Fidelity: 0.9706998318603844\n",
            "Epoch Number: 1180, Loss: 0.0008509181962779936, Fidelity: 0.9708294978398041\n",
            "Epoch Number: 1200, Loss: 0.0008436287898968501, Fidelity: 0.970954711399319\n",
            "Epoch Number: 1220, Loss: 0.0008366167357985259, Fidelity: 0.9710756722498426\n",
            "Epoch Number: 1240, Loss: 0.0008298682071978433, Fidelity: 0.9711925668065021\n",
            "Epoch Number: 1260, Loss: 0.0008233703445319326, Fidelity: 0.9713055694509904\n",
            "Epoch Number: 1280, Loss: 0.0008171111642003247, Fidelity: 0.9714148436386938\n",
            "Epoch Number: 1300, Loss: 0.0008110794764857425, Fidelity: 0.9715205429039502\n",
            "Epoch Number: 1320, Loss: 0.0008052648136959987, Fidelity: 0.9716228117373127\n",
            "Epoch Number: 1340, Loss: 0.000799657365804995, Fidelity: 0.9717217863752854\n",
            "Epoch Number: 1360, Loss: 0.0007942479227010139, Fidelity: 0.9718175955124299\n",
            "Epoch Number: 1380, Loss: 0.0007890278238530625, Fidelity: 0.9719103609162905\n",
            "Epoch Number: 1400, Loss: 0.0007839889119120912, Fidelity: 0.9720001980022699\n",
            "Epoch Number: 1420, Loss: 0.0007791234924311594, Fidelity: 0.9720872163260065\n",
            "Epoch Number: 1440, Loss: 0.0007744242971950038, Fidelity: 0.9721715200344143\n",
            "Epoch Number: 1460, Loss: 0.0007698844512515213, Fidelity: 0.9722532082710177\n",
            "Epoch Number: 1480, Loss: 0.0007654974434996477, Fidelity: 0.9723323755356618\n",
            "Epoch Number: 1500, Loss: 0.0007612571001027684, Fidelity: 0.9724091120095281\n",
            "Epoch Number: 1520, Loss: 0.0007571575606156046, Fidelity: 0.972483503845591\n",
            "Epoch Number: 1540, Loss: 0.0007531932559214466, Fidelity: 0.9725556334392385\n",
            "Epoch Number: 1560, Loss: 0.0007493588892155372, Fidelity: 0.9726255796551683\n",
            "Epoch Number: 1580, Loss: 0.0007456494174071153, Fidelity: 0.9726934180570487\n",
            "Epoch Number: 1600, Loss: 0.0007420600353927251, Fidelity: 0.9727592210942358\n",
            "Epoch Number: 1620, Loss: 0.0007385861609509781, Fidelity: 0.9728230582855433\n",
            "Epoch Number: 1640, Loss: 0.0007352234212370991, Fidelity: 0.9728849963813925\n",
            "Epoch Number: 1660, Loss: 0.0007319676405998457, Fidelity: 0.9729450995085946\n",
            "Epoch Number: 1680, Loss: 0.0007288148288380049, Fidelity: 0.9730034293133738\n",
            "Epoch Number: 1700, Loss: 0.0007257611709853229, Fidelity: 0.9730600450819731\n",
            "Epoch Number: 1720, Loss: 0.0007228030173065937, Fidelity: 0.9731150038626264\n",
            "Epoch Number: 1740, Loss: 0.0007199368745971596, Fidelity: 0.973168360568218\n",
            "Epoch Number: 1760, Loss: 0.0007171593979051447, Fidelity: 0.9732201680754874\n",
            "Epoch Number: 1780, Loss: 0.0007144673827138671, Fidelity: 0.9732704773197525\n",
            "Epoch Number: 1800, Loss: 0.0007118577577262684, Fidelity: 0.9733193373821738\n",
            "Epoch Number: 1820, Loss: 0.0007093275782721686, Fidelity: 0.9733667955688361\n",
            "Epoch Number: 1840, Loss: 0.0007068740201926005, Fidelity: 0.9734128974840695\n",
            "Epoch Number: 1860, Loss: 0.0007044943738253584, Fidelity: 0.9734576871048253\n",
            "Epoch Number: 1880, Loss: 0.0007021860385804818, Fidelity: 0.9735012068467169\n",
            "Epoch Number: 1900, Loss: 0.000699946517879889, Fidelity: 0.9735434976257274\n",
            "Epoch Number: 1920, Loss: 0.0006977734141867921, Fidelity: 0.9735845989205768\n",
            "Epoch Number: 1940, Loss: 0.0006956644246963789, Fidelity: 0.9736245488247807\n",
            "Epoch Number: 1960, Loss: 0.0006936173368230544, Fidelity: 0.9736633841045769\n",
            "Epoch Number: 1980, Loss: 0.000691630024324533, Fidelity: 0.973701140246685\n",
            "Epoch Number: 2000, Loss: 0.0006897004433529043, Fidelity: 0.9737378515091986\n"
          ]
        }
      ],
      "source": [
        "loss_array = []\n",
        "fid_array = []\n",
        "\n",
        "#Run the optimization\n",
        "for epoch in range(num_epochs):\n",
        "  loss_val, fid_val, opt_state, (theta, phi, D, chi_1, chi_2) = update(0, theta, phi, D, chi_1, chi_2, opt_state, init_state, target_state)\n",
        "  loss_array.append(loss_val); fid_array.append(fid_val)\n",
        "  if (epoch%20) == 0:\n",
        "    print (f\"Epoch Number: {epoch}, Loss: {loss_val}, Fidelity: {fid_val}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
